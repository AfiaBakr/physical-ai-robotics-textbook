# Data Model: Vision-Language-Action (VLA) Module

## Overview
This document defines the data structures and entities used in the Vision-Language-Action (VLA) system for Physical AI & Humanoid Robotics. The data model encompasses the flow of information from speech input to robot action execution.

## Core Entities

### 1. SpokenCommand
Represents the raw spoken input from a human user.

```python
class SpokenCommand:
    id: str                    # Unique identifier for the command
    audio_data: bytes          # Raw audio data or file path
    timestamp: datetime        # Time when command was received
    confidence: float          # Confidence score from ASR system (0.0-1.0)
    raw_text: str              # Raw text from speech recognition
    source_device: str         # Device identifier that captured the audio
    environment_context: dict  # Context about the environment when command was given
```

### 2. ParsedCommand
Represents the structured interpretation of a spoken command.

```python
class ParsedCommand:
    id: str                    # Unique identifier (references SpokenCommand.id)
    structured_intent: str     # High-level intent (e.g., "grasp_object", "navigate_to")
    entities: List[Entity]     # Extracted entities from the command
    action_sequence: List[Action]  # Sequence of actions derived from the command
    confidence: float          # Confidence in the parsed interpretation (0.0-1.0)
    ambiguity_flags: List[str] # Indicators of ambiguous elements in the command
    clarification_needed: bool # Whether human clarification is required
```

### 3. Entity
Represents named entities extracted from the command (objects, locations, etc.).

```python
class Entity:
    entity_type: str           # Type of entity (e.g., "object", "location", "person")
    value: str                 # Actual value of the entity (e.g., "red ball", "kitchen")
    confidence: float          # Confidence in entity identification (0.0-1.0)
    attributes: Dict[str, Any] # Additional attributes (color, size, etc.)
    spatial_reference: Optional[SpatialReference]  # Spatial relationship if applicable
```

### 4. SpatialReference
Represents spatial relationships between entities.

```python
class SpatialReference:
    reference_frame: str       # Coordinate frame (e.g., "robot_base", "world_map")
    position: Vector3          # 3D position (x, y, z)
    orientation: Quaternion    # Orientation (x, y, z, w)
    uncertainty: Matrix3x3     # Uncertainty covariance matrix
    timestamp: datetime        # Time when spatial data was computed
```

### 5. Action
Represents a single robot action derived from the command.

```python
class Action:
    action_id: str             # Unique identifier for this action
    action_type: str           # Type of action (e.g., "move_to", "grasp", "pick_up")
    parameters: Dict[str, Any] # Action-specific parameters
    priority: int              # Priority level (0=highest, higher=lower)
    preconditions: List[str]   # Conditions that must be true before execution
    effects: List[str]         # Expected effects of the action
    duration_estimate: float   # Estimated time to complete the action (seconds)
    success_criteria: List[str] # Criteria for determining successful completion
```

### 6. CognitivePlan
Represents the high-level plan generated by the LLM-based cognitive planner.

```python
class CognitivePlan:
    plan_id: str               # Unique identifier for the plan
    original_command: str      # Original command that generated this plan
    plan_steps: List[PlanStep] # Ordered sequence of steps to achieve the goal
    overall_goal: str          # High-level goal of the plan
    estimated_duration: float  # Total estimated time for plan execution
    risk_assessment: RiskAssessment  # Risk evaluation for the plan
    alternative_paths: List[List[PlanStep]]  # Alternative execution paths
    context: Dict[str, Any]    # Environmental and situational context
```

### 7. PlanStep
Represents a single step within a cognitive plan.

```python
class PlanStep:
    step_id: str               # Unique identifier for this step
    description: str           # Natural language description of the step
    required_actions: List[Action]  # Actions needed to complete this step
    prerequisites: List[str]   # Prerequisites that must be satisfied
    expected_outcomes: List[str] # Expected outcomes of this step
    failure_recovery: List[Action]  # Actions to take if this step fails
    confidence: float          # Planner's confidence in this step (0.0-1.0)
```

### 8. VisionData
Represents processed visual information from the robot's sensors.

```python
class VisionData:
    capture_time: datetime     # Time when the image was captured
    image_data: bytes          # Raw image data or reference to image
    detected_objects: List[DetectedObject]  # Objects detected in the scene
    scene_description: str     # Natural language description of the scene
    spatial_map: SpatialMap    # Map of objects and their spatial relationships
    confidence_threshold: float # Minimum confidence for object detection
    processing_pipeline: str   # Which vision pipeline was used
```

### 9. DetectedObject
Represents an individual object detected in the visual scene.

```python
class DetectedObject:
    object_id: str             # Unique identifier for this object instance
    class_label: str           # Semantic class of the object (e.g., "ball", "cup")
    bounding_box: BoundingBox  # 2D bounding box in the image
    mask: Optional[bytes]      # Segmentation mask if available
    pose: SpatialReference     # 3D pose of the object in world coordinates
    confidence: float          # Detection confidence (0.0-1.0)
    properties: Dict[str, Any] # Additional properties (color, material, etc.)
    relationships: List[ObjectRelationship]  # Relationships with other objects
```

### 10. BoundingBox
Represents a 2D bounding box around an object in an image.

```python
class BoundingBox:
    x_min: float               # Normalized minimum X coordinate (0.0-1.0)
    y_min: float               # Normalized minimum Y coordinate (0.0-1.0)
    x_max: float               # Normalized maximum X coordinate (0.0-1.0)
    y_max: float               # Normalized maximum Y coordinate (0.0-1.0)
    width: float               # Width of the bounding box
    height: float              # Height of the bounding box
```

### 11. ObjectRelationship
Represents spatial or semantic relationships between objects.

```python
class ObjectRelationship:
    subject_id: str            # ID of the subject object
    predicate: str             # Relationship type (e.g., "on_top_of", "next_to", "inside")
    object_id: str             # ID of the object the relationship refers to
    strength: float            # Strength or certainty of the relationship (0.0-1.0)
    spatial_constraints: Dict[str, float]  # Spatial constraints of the relationship
```

### 12. SpatialMap
Represents the spatial layout of objects in the environment.

```python
class SpatialMap:
    map_id: str                # Unique identifier for this spatial map
    objects: Dict[str, SpatialReference]  # Mapping of object IDs to positions
    landmarks: Dict[str, SpatialReference]  # Fixed landmarks in the environment
    navigation_points: List[SpatialReference]  # Points that are safe for navigation
    obstacles: List[SpatialReference]  # Static obstacles in the environment
    coordinate_frame: str      # Reference frame for all positions
    last_updated: datetime     # Time when the map was last updated
```

### 13. ROS2ActionRequest
Represents a request to execute a robot action through ROS 2.

```python
class ROS2ActionRequest:
    request_id: str            # Unique identifier for this request
    action_name: str           # Name of the ROS 2 action server
    action_goal: Dict[str, Any] # Parameters for the action goal
    timeout: float             # Maximum time to wait for completion (seconds)
    feedback_callback: str     # Callback to handle action feedback
    result_callback: str       # Callback to handle action result
    cancellation_token: str    # Token to cancel the action if needed
    priority: int              # Priority for action execution queue
```

### 14. ActionExecutionResult
Represents the result of executing a robot action.

```python
class ActionExecutionResult:
    request_id: str            # ID of the original request
    action_name: str           # Name of the action that was executed
    success: bool              # Whether the action completed successfully
    result_data: Dict[str, Any] # Detailed result data from the action
    execution_time: float      # Time taken to execute the action (seconds)
    error_code: Optional[str]  # Error code if the action failed
    error_message: Optional[str] # Detailed error message if the action failed
    side_effects: List[str]    # Any unexpected side effects observed
```

### 15. VLAPipelineState
Represents the current state of the entire VLA pipeline.

```python
class VLAPipelineState:
    state_id: str              # Unique identifier for this state snapshot
    current_stage: str         # Current stage of the pipeline (e.g., "listening", "processing", "executing")
    active_command: Optional[str]  # ID of the currently processing command
    current_plan: Optional[str]    # ID of the current cognitive plan
    robot_status: RobotStatus     # Current status of the robot
    vision_data: Optional[str]    # ID of the most recent vision data
    last_error: Optional[str]     # Last error encountered in the pipeline
    timestamp: datetime        # Time when this state was recorded
    confidence_level: float    # Overall confidence in the current state (0.0-1.0)
```

### 16. RobotStatus
Represents the current status of the robot.

```python
class RobotStatus:
    battery_level: float       # Battery charge level (0.0-1.0)
    operational: bool          # Whether the robot is operational
    current_pose: SpatialReference  # Current position and orientation of the robot
    joint_states: Dict[str, float]  # Current state of all joints
    gripper_state: str         # Current state of the gripper ("open", "closed", "moving")
    active_actions: List[str]  # Currently executing action IDs
    safety_status: SafetyStatus  # Current safety status
    communication_status: str  # Status of communication with control system
```

### 17. SafetyStatus
Represents the safety status of the robot and its environment.

```python
class SafetyStatus:
    emergency_stop: bool       # Whether emergency stop is activated
    collision_risk: float      # Risk of collision (0.0-1.0)
    human_proximity: float     # Proximity to humans (0.0-1.0, higher = closer)
    environment_safe: bool     # Whether environment is deemed safe for operation
    action_approved: bool      # Whether current action is approved for execution
    safety_override: bool      # Whether safety constraints are overridden
    safety_violations: List[str]  # List of recent safety violations
```

### 18. RiskAssessment
Represents the risk evaluation of a cognitive plan.

```python
class RiskAssessment:
    probability_of_failure: float  # Estimated probability of plan failure (0.0-1.0)
    potential_damage: str      # Potential damage if plan fails (e.g., "low", "medium", "high")
    safety_risks: List[str]    # Specific safety risks identified
    resource_consumption: Dict[str, float]  # Estimated resource consumption
    mitigation_strategies: List[str]  # Strategies to mitigate identified risks
    risk_score: float          # Overall risk score (0.0-1.0)
```

## Data Flow Diagrams

### High-Level Data Flow
```
SpokenCommand → ParsedCommand → CognitivePlan → VisionData → ROS2ActionRequest → ActionExecutionResult
```

### Detailed Data Flow
```
SpokenCommand
    ↓ (ASR processing)
AudioData → RawText → Entities → ParsedCommand
    ↓ (LLM planning)
CognitivePlan ← PlanSteps ← RiskAssessment
    ↓ (Vision processing)
VisionData ← DetectedObjects ← SpatialMap
    ↓ (Action coordination)
ROS2ActionRequest → RobotAction → ActionExecutionResult
    ↓ (State management)
VLAPipelineState
```

## Validation Rules

### 1. SpokenCommand Validation
- `confidence` must be between 0.0 and 1.0
- `raw_text` must not be empty
- `timestamp` must be in the past

### 2. Action Validation
- `priority` must be a non-negative integer
- `duration_estimate` must be positive
- `action_type` must be one of the supported action types

### 3. SpatialReference Validation
- Position coordinates must be finite numbers
- Quaternion must be normalized (unit length)
- Timestamp must be in the past

### 4. CognitivePlan Validation
- `plan_steps` must not be empty
- `estimated_duration` must be positive
- `overall_goal` must not be empty

### 5. VisionData Validation
- `confidence_threshold` must be between 0.0 and 1.0
- `detected_objects` should not exceed reasonable limits (e.g., <1000 objects)
- `capture_time` must be in the past